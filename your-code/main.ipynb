{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "from lxml import html\n",
    "from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "# from urllib.request import urlopen\n",
    "# import random\n",
    "# import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "github= requests.get('https://github.com/trending/developers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "github.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_github= BeautifulSoup(github.content)\n",
    "#print (soup_github)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yair Morgenstern',\n",
       " 'Rich Harris',\n",
       " 'Tapas Adhikary',\n",
       " 'Paul Bottein',\n",
       " 'Florian Rival',\n",
       " 'metonym',\n",
       " 'George Hotz',\n",
       " 'Milad akarie',\n",
       " 'Emil Ernerfeldt',\n",
       " 'David Sherret',\n",
       " 'Brian Vaughn',\n",
       " 'Maximilian Roos',\n",
       " 'Andrew Kane',\n",
       " 'Waldemar Hummer',\n",
       " 'Zachary Mueller',\n",
       " 'Jess Frazelle',\n",
       " 'Suyeol Jeon',\n",
       " 'Connor Ferster',\n",
       " '0xAX',\n",
       " 'Eren Gölge',\n",
       " 'Luong Vo',\n",
       " 'Andy Stewart',\n",
       " 'Leo',\n",
       " 'Kyle Fuller',\n",
       " 'Jonny Borges']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "developers= [texto.text.strip('\\n ') for texto in soup_github.select('h1.h3.lh-condensed > a')]\n",
    "developers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nuclei-templates',\n",
       " 'nannyml',\n",
       " 'TopFreeProxies',\n",
       " 'freqtrade',\n",
       " 'transformers',\n",
       " 'linux-insides',\n",
       " 'Python-100-Days',\n",
       " 'reloadium',\n",
       " 'CyberControllerServer',\n",
       " 'core',\n",
       " 'CleanMyWechat',\n",
       " 'tutorials',\n",
       " 'ungoogled-chromium',\n",
       " 'PayloadsAllTheThings',\n",
       " 'denoising-diffusion-pytorch',\n",
       " 'public-apis',\n",
       " 'scenic',\n",
       " 'RsaCtfTool',\n",
       " 'demucs',\n",
       " 'YukkiMusicBot',\n",
       " 'akshare',\n",
       " 'gibMacOS',\n",
       " 'archinstall',\n",
       " 'proxy_pool',\n",
       " 'accelerate']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "repositories= requests.get('https://github.com/trending/python?since=daily')\n",
    "soup_repos= BeautifulSoup(repositories.content)\n",
    "\n",
    "lista_repos= [texto['href'].split('/')[-1] for texto in soup_repos.select('h1.h3.lh-condensed > a')]\n",
    "lista_repos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Walt_Disney_envelope_ca._1921.jpg',\n",
       " 'Trolley_Troubles_poster.jpg',\n",
       " 'Steamboat-willie.jpg',\n",
       " 'Walt_Disney_1935.jpg',\n",
       " 'Walt_Disney_Snow_white_1937_trailer_screenshot_(13).jpg',\n",
       " 'Disney_drawing_goofy.jpg',\n",
       " 'DisneySchiphol1951.jpg',\n",
       " 'WaltDisneyplansDisneylandDec1954.jpg',\n",
       " 'Walt_disney_portrait_right.jpg',\n",
       " 'Walt_Disney_Grave.JPG',\n",
       " 'Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " 'Disney_Oscar_1953_(cropped).jpg',\n",
       " 'Disney1968.jpg']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "disney= requests.get('https://en.wikipedia.org/wiki/Walt_Disney')\n",
    "soup_disney=  BeautifulSoup(disney.content)\n",
    "\n",
    "lista_linkdisney= [texto['href'].split(':')[-1] for texto in soup_disney.select('div.thumbinner > a.image')]\n",
    "lista_linkdisney"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#Snakes',\n",
       " '#Computing',\n",
       " '#People',\n",
       " '#Roller_coasters',\n",
       " '#Vehicles',\n",
       " '#Weaponry',\n",
       " '#Other_uses',\n",
       " '#See_also',\n",
       " '/wiki/Pythonidae',\n",
       " '/wiki/Python_(genus)',\n",
       " '/wiki/Python_(mythology)',\n",
       " '/wiki/Python_(programming_language)',\n",
       " '/wiki/CMU_Common_Lisp',\n",
       " '/wiki/PERQ#PERQ_3',\n",
       " '/wiki/Python_of_Aenus',\n",
       " '/wiki/Python_(painter)',\n",
       " '/wiki/Python_of_Byzantium',\n",
       " '/wiki/Python_of_Catana',\n",
       " '/wiki/Python_Anghelo',\n",
       " '/wiki/Python_(Efteling)',\n",
       " '/wiki/Python_(Busch_Gardens_Tampa_Bay)',\n",
       " '/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)',\n",
       " '/wiki/Python_(automobile_maker)',\n",
       " '/wiki/Python_(Ford_prototype)',\n",
       " '/wiki/Python_(missile)',\n",
       " '/wiki/Python_(nuclear_primary)',\n",
       " '/wiki/Colt_Python',\n",
       " '/wiki/PYTHON',\n",
       " '/wiki/Python_(film)',\n",
       " '/wiki/Monty_Python',\n",
       " '/wiki/Python_(Monty)_Pictures',\n",
       " '/wiki/Timon_of_Phlius',\n",
       " '/wiki/Cython',\n",
       " '/wiki/Pithon']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "python= requests.get('https://en.wikipedia.org/wiki/Python')\n",
    "soup_python= BeautifulSoup(python.content)\n",
    "\n",
    "links_python= [texto['href'] for texto in soup_python.select('div.mw-parser-output  ul > li > a ')]\n",
    "links_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "code= requests.get('http://uscode.house.gov/download/download.shtml')\n",
    "soup_code= BeautifulSoup(code.content)\n",
    "\n",
    "len(soup_code.select('div.usctitlechanged'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALEJANDRO ROSALES CASTILLO',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'JASON DEREK BROWN',\n",
       " 'ALEXIS FLORES',\n",
       " 'JOSE RODOLFO VILLARREAL-HERNANDEZ',\n",
       " 'RAFAEL CARO-QUINTERO',\n",
       " 'YULAN ADONAY ARCHAGA CARIAS',\n",
       " 'EUGENE PALMER',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'OCTAVIANO JUAREZ-CORRO']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code \n",
    "fbi= requests.get('https://www.fbi.gov/wanted/topten')\n",
    "soup_fbi= BeautifulSoup(fbi.content)\n",
    "top_ten= [nombre.text for nombre in soup_fbi.select('h3.title > a')]\n",
    "top_ten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Date &amp; Time UTC</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Latitude degrees</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Longitude degrees</th>\n",
       "      <th>Region name [+]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>12345678910»</th>\n",
       "      <th>12345678910»</th>\n",
       "      <th>12345678910».1</th>\n",
       "      <th>12345678910»</th>\n",
       "      <th>12345678910».1</th>\n",
       "      <th>12345678910»</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05-15 23:23:49.726min ago</td>\n",
       "      <td>61.73</td>\n",
       "      <td>N</td>\n",
       "      <td>150.80</td>\n",
       "      <td>W</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-05-15 23:04:44.045min ago</td>\n",
       "      <td>20.91</td>\n",
       "      <td>S</td>\n",
       "      <td>69.12</td>\n",
       "      <td>W</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-05-15 22:31:37.01hr 18min ago</td>\n",
       "      <td>4.86</td>\n",
       "      <td>S</td>\n",
       "      <td>102.85</td>\n",
       "      <td>E</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-05-15 22:13:46.61hr 36min ago</td>\n",
       "      <td>35.52</td>\n",
       "      <td>N</td>\n",
       "      <td>3.74</td>\n",
       "      <td>W</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-05-15 22:11:49.01hr 38min ago</td>\n",
       "      <td>30.89</td>\n",
       "      <td>S</td>\n",
       "      <td>65.30</td>\n",
       "      <td>W</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-05-15 22:02:02.01hr 48min ago</td>\n",
       "      <td>0.89</td>\n",
       "      <td>N</td>\n",
       "      <td>98.74</td>\n",
       "      <td>E</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-05-15 21:50:00.02hr 00min ago</td>\n",
       "      <td>63.88</td>\n",
       "      <td>N</td>\n",
       "      <td>22.56</td>\n",
       "      <td>W</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-05-15 21:49:56.72hr 00min ago</td>\n",
       "      <td>36.56</td>\n",
       "      <td>N</td>\n",
       "      <td>28.55</td>\n",
       "      <td>E</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-05-15 21:15:41.02hr 34min ago</td>\n",
       "      <td>6.83</td>\n",
       "      <td>N</td>\n",
       "      <td>127.62</td>\n",
       "      <td>E</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-05-15 21:13:25.22hr 36min ago</td>\n",
       "      <td>35.48</td>\n",
       "      <td>N</td>\n",
       "      <td>3.78</td>\n",
       "      <td>W</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-05-15 21:12:17.02hr 37min ago</td>\n",
       "      <td>32.12</td>\n",
       "      <td>S</td>\n",
       "      <td>68.59</td>\n",
       "      <td>W</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022-05-15 21:03:42.02hr 46min ago</td>\n",
       "      <td>8.13</td>\n",
       "      <td>S</td>\n",
       "      <td>107.88</td>\n",
       "      <td>E</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022-05-15 20:59:18.02hr 50min ago</td>\n",
       "      <td>8.18</td>\n",
       "      <td>S</td>\n",
       "      <td>107.87</td>\n",
       "      <td>E</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022-05-15 20:55:13.02hr 54min ago</td>\n",
       "      <td>8.45</td>\n",
       "      <td>S</td>\n",
       "      <td>121.27</td>\n",
       "      <td>E</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2022-05-15 20:26:10.33hr 23min ago</td>\n",
       "      <td>35.50</td>\n",
       "      <td>N</td>\n",
       "      <td>3.74</td>\n",
       "      <td>W</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022-05-15 20:21:09.03hr 28min ago</td>\n",
       "      <td>18.35</td>\n",
       "      <td>S</td>\n",
       "      <td>69.37</td>\n",
       "      <td>W</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022-05-15 20:18:05.03hr 31min ago</td>\n",
       "      <td>22.05</td>\n",
       "      <td>S</td>\n",
       "      <td>68.89</td>\n",
       "      <td>W</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-05-15 20:14:34.03hr 35min ago</td>\n",
       "      <td>36.94</td>\n",
       "      <td>S</td>\n",
       "      <td>177.64</td>\n",
       "      <td>E</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022-05-15 19:56:00.03hr 54min ago</td>\n",
       "      <td>19.23</td>\n",
       "      <td>N</td>\n",
       "      <td>155.39</td>\n",
       "      <td>W</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2022-05-15 19:54:19.03hr 55min ago</td>\n",
       "      <td>30.45</td>\n",
       "      <td>N</td>\n",
       "      <td>114.02</td>\n",
       "      <td>W</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Date & Time UTC Latitude degrees                  \\\n",
       "                         12345678910»    12345678910» 12345678910».1   \n",
       "0       2022-05-15 23:23:49.726min ago            61.73               N   \n",
       "1       2022-05-15 23:04:44.045min ago            20.91               S   \n",
       "2   2022-05-15 22:31:37.01hr 18min ago             4.86               S   \n",
       "3   2022-05-15 22:13:46.61hr 36min ago            35.52               N   \n",
       "4   2022-05-15 22:11:49.01hr 38min ago            30.89               S   \n",
       "5   2022-05-15 22:02:02.01hr 48min ago             0.89               N   \n",
       "6   2022-05-15 21:50:00.02hr 00min ago            63.88               N   \n",
       "7   2022-05-15 21:49:56.72hr 00min ago            36.56               N   \n",
       "8   2022-05-15 21:15:41.02hr 34min ago             6.83               N   \n",
       "9   2022-05-15 21:13:25.22hr 36min ago            35.48               N   \n",
       "10  2022-05-15 21:12:17.02hr 37min ago            32.12               S   \n",
       "11  2022-05-15 21:03:42.02hr 46min ago             8.13               S   \n",
       "12  2022-05-15 20:59:18.02hr 50min ago             8.18               S   \n",
       "13  2022-05-15 20:55:13.02hr 54min ago             8.45               S   \n",
       "14  2022-05-15 20:26:10.33hr 23min ago            35.50               N   \n",
       "15  2022-05-15 20:21:09.03hr 28min ago            18.35               S   \n",
       "16  2022-05-15 20:18:05.03hr 31min ago            22.05               S   \n",
       "17  2022-05-15 20:14:34.03hr 35min ago            36.94               S   \n",
       "18  2022-05-15 19:56:00.03hr 54min ago            19.23               N   \n",
       "19  2022-05-15 19:54:19.03hr 55min ago            30.45               N   \n",
       "\n",
       "   Longitude degrees                 Region name [+]  \n",
       "       12345678910» 12345678910».1   12345678910»  \n",
       "0             150.80               W             3.1  \n",
       "1              69.12               W             3.1  \n",
       "2             102.85               E             3.3  \n",
       "3               3.74               W             1.8  \n",
       "4              65.30               W             3.0  \n",
       "5              98.74               E             3.0  \n",
       "6              22.56               W             3.2  \n",
       "7              28.55               E             3.7  \n",
       "8             127.62               E             3.7  \n",
       "9               3.78               W             1.8  \n",
       "10             68.59               W             3.2  \n",
       "11            107.88               E             2.7  \n",
       "12            107.87               E             2.8  \n",
       "13            121.27               E             2.9  \n",
       "14              3.74               W             2.0  \n",
       "15             69.37               W             3.6  \n",
       "16             68.89               W             2.6  \n",
       "17            177.64               E             3.1  \n",
       "18            155.39               W             2.1  \n",
       "19            114.02               W             4.2  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "\n",
    "html = requests.get(url).content\n",
    "df_list= pd.read_html(html)\n",
    "\n",
    "df = df_list[-1]\n",
    "df\n",
    "df[['Date & Time UTC', 'Latitude degrees', 'Longitude degrees', 'Region name [+]']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, days, title, city, country of next 25 hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://mlh.io/seasons/na-2020/events'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date - Days</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HackMTY</td>\n",
       "      <td>Aug 24th - 25th</td>\n",
       "      <td>Monterrey,</td>\n",
       "      <td>MX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Citizen Hacks</td>\n",
       "      <td>Sep 6th - 8th</td>\n",
       "      <td>Toronto,</td>\n",
       "      <td>ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PennApps</td>\n",
       "      <td>Sep 6th - 8th</td>\n",
       "      <td>Philadelphia,</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hackathon de Futuras Tecnologías</td>\n",
       "      <td>Sep 7th - 8th</td>\n",
       "      <td>Torreón,</td>\n",
       "      <td>MX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hack the North</td>\n",
       "      <td>Sep 13th - 15th</td>\n",
       "      <td>Waterloo,</td>\n",
       "      <td>ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HopHacks</td>\n",
       "      <td>Sep 13th - 15th</td>\n",
       "      <td>Baltimore,</td>\n",
       "      <td>MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BigRed//Hacks</td>\n",
       "      <td>Sep 20th - 22nd</td>\n",
       "      <td>Ithaca,</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HackRice</td>\n",
       "      <td>Sep 20th - 21st</td>\n",
       "      <td>Houston,</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SBUHacks</td>\n",
       "      <td>Sep 20th - 21st</td>\n",
       "      <td>Stony Brook,</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ShellHacks</td>\n",
       "      <td>Sep 20th - 22nd</td>\n",
       "      <td>Miami,</td>\n",
       "      <td>FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sunhacks</td>\n",
       "      <td>Sep 20th - 22nd</td>\n",
       "      <td>Tempe,</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Kent Hack Enough</td>\n",
       "      <td>Sep 27th - 29th</td>\n",
       "      <td>Kent,</td>\n",
       "      <td>OH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MedHacks</td>\n",
       "      <td>Sep 27th - 29th</td>\n",
       "      <td>Baltimore,</td>\n",
       "      <td>MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>VolHacks</td>\n",
       "      <td>Sep 27th - 29th</td>\n",
       "      <td>Knoxville,</td>\n",
       "      <td>TN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GirlHacks</td>\n",
       "      <td>Sep 28th - 29th</td>\n",
       "      <td>Newark,</td>\n",
       "      <td>NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>GrizzHacks 4</td>\n",
       "      <td>Sep 28th - 29th</td>\n",
       "      <td>Rochester,</td>\n",
       "      <td>MI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hackNY</td>\n",
       "      <td>Sep 28th - 29th</td>\n",
       "      <td>New York City,</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>HackUMBC</td>\n",
       "      <td>Sep 28th - 29th</td>\n",
       "      <td>Baltimore,</td>\n",
       "      <td>MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>RamHacks</td>\n",
       "      <td>Sep 28th - 29th</td>\n",
       "      <td>Richmond,</td>\n",
       "      <td>VA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>HackTheU</td>\n",
       "      <td>Oct 5th - 6th</td>\n",
       "      <td>Salt Lake City,</td>\n",
       "      <td>UT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MHacks</td>\n",
       "      <td>Oct 11th - 13th</td>\n",
       "      <td>Ann Arbor,</td>\n",
       "      <td>MI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Polyhack</td>\n",
       "      <td>Oct 11th - 12th</td>\n",
       "      <td>Somerville,</td>\n",
       "      <td>MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>The Pitt Challenge</td>\n",
       "      <td>Oct 11th - 12th</td>\n",
       "      <td>Pittsburgh,</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Dubhacks</td>\n",
       "      <td>Oct 12th - 13th</td>\n",
       "      <td>Seattle,</td>\n",
       "      <td>WA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>HackNC</td>\n",
       "      <td>Oct 12th - 13th</td>\n",
       "      <td>Chapel Hill,</td>\n",
       "      <td>NC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Title              Date - Days  \\\n",
       "0                             HackMTY  Aug 24th - 25th           \n",
       "1                       Citizen Hacks    Sep 6th - 8th           \n",
       "2                            PennApps    Sep 6th - 8th           \n",
       "3   Hackathon de Futuras Tecnologías     Sep 7th - 8th           \n",
       "4                      Hack the North  Sep 13th - 15th           \n",
       "5                            HopHacks  Sep 13th - 15th           \n",
       "6                       BigRed//Hacks  Sep 20th - 22nd           \n",
       "7                            HackRice  Sep 20th - 21st           \n",
       "8                           SBUHacks   Sep 20th - 21st           \n",
       "9                          ShellHacks  Sep 20th - 22nd           \n",
       "10                           sunhacks  Sep 20th - 22nd           \n",
       "11                   Kent Hack Enough  Sep 27th - 29th           \n",
       "12                           MedHacks  Sep 27th - 29th           \n",
       "13                           VolHacks  Sep 27th - 29th           \n",
       "14                          GirlHacks  Sep 28th - 29th           \n",
       "15                       GrizzHacks 4  Sep 28th - 29th           \n",
       "16                             hackNY  Sep 28th - 29th           \n",
       "17                           HackUMBC  Sep 28th - 29th           \n",
       "18                           RamHacks  Sep 28th - 29th           \n",
       "19                           HackTheU    Oct 5th - 6th           \n",
       "20                             MHacks  Oct 11th - 13th           \n",
       "21                           Polyhack  Oct 11th - 12th           \n",
       "22                 The Pitt Challenge  Oct 11th - 12th           \n",
       "23                           Dubhacks  Oct 12th - 13th           \n",
       "24                             HackNC  Oct 12th - 13th           \n",
       "\n",
       "               City       Country  \n",
       "0        Monterrey,            MX  \n",
       "1          Toronto,            ON  \n",
       "2     Philadelphia,            PA  \n",
       "3          Torreón,            MX  \n",
       "4         Waterloo,            ON  \n",
       "5        Baltimore,            MD  \n",
       "6           Ithaca,            NY  \n",
       "7          Houston,            TX  \n",
       "8      Stony Brook,            NY  \n",
       "9            Miami,            FL  \n",
       "10           Tempe,            AZ  \n",
       "11            Kent,            OH  \n",
       "12       Baltimore,            MD  \n",
       "13       Knoxville,            TN  \n",
       "14          Newark,            NJ  \n",
       "15       Rochester,            MI  \n",
       "16   New York City,            NY  \n",
       "17       Baltimore,            MD  \n",
       "18        Richmond,            VA  \n",
       "19  Salt Lake City,            UT  \n",
       "20       Ann Arbor,            MI  \n",
       "21      Somerville,            MA  \n",
       "22      Pittsburgh,            PA  \n",
       "23         Seattle,            WA  \n",
       "24     Chapel Hill,            NC  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "event_page=requests.get(url)\n",
    "soup_event=BeautifulSoup(event_page.content)\n",
    "soup_event.select('div.row > div.event')[0].text.strip(' \\n').split('\\n')\n",
    "row = [x.text.strip(' \\n').split('\\n') for x in soup_event.select('div.row > div.event')]\n",
    "row[0]\n",
    "data = row\n",
    "df = pd.DataFrame(data)\n",
    "df.head()\n",
    "df_edit = df[[0,1,5,6]]\n",
    "df_edit.rename(columns = {0 : 'Title', 1 : 'Date - Days', 5 : 'City', 6 : 'Country'}).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "English\n",
      "6 458 000+ articles\n",
      "\n",
      "\n",
      "日本語\n",
      "1 314 000+ 記事\n",
      "\n",
      "\n",
      "Русский\n",
      "1 798 000+ статей\n",
      "\n",
      "\n",
      "Español\n",
      "1 755 000+ artículos\n",
      "\n",
      "\n",
      "Deutsch\n",
      "2 667 000+ Artikel\n",
      "\n",
      "\n",
      "Français\n",
      "2 400 000+ articles\n",
      "\n",
      "\n",
      "Italiano\n",
      "1 742 000+ voci\n",
      "\n",
      "\n",
      "中文\n",
      "1 256 000+ 条目 / 條目\n",
      "\n",
      "\n",
      "Português\n",
      "1 085 000+ artigos\n",
      "\n",
      "\n",
      "Polski\n",
      "1 512 000+ haseł\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#your code\n",
    "\n",
    "html = requests.get(url).content\n",
    "soup_lan = BeautifulSoup(html, \"html.parser\")\n",
    "list_lan = soup_lan.find_all('a', {'class' : 'link-box'})\n",
    "for x in list_lan:\n",
    "    print(x.get_text())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport',\n",
       " 'Digital service performance',\n",
       " 'Government reference data']"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code \n",
    "html = requests.get(url).content\n",
    "soup_datasets = BeautifulSoup(html, \"lxml\")\n",
    "list_datasets= [texto.text for texto in soup_datasets.select('h3 > a.govuk-link')]\n",
    "list_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Native Speakers(millions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>929.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>474.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>372.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi (sanskritised Hindustani)[11]</td>\n",
       "      <td>343.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>233.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>232.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Russian</td>\n",
       "      <td>154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>125.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Western Punjabi[12]</td>\n",
       "      <td>92.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Yue Chinese</td>\n",
       "      <td>85.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Language Native Speakers(millions)\n",
       "0                     Mandarin Chinese                     929.0\n",
       "1                              Spanish                     474.7\n",
       "2                              English                     372.9\n",
       "3  Hindi (sanskritised Hindustani)[11]                     343.9\n",
       "4                              Bengali                     233.7\n",
       "5                           Portuguese                     232.4\n",
       "6                              Russian                     154.0\n",
       "7                             Japanese                     125.3\n",
       "8                  Western Punjabi[12]                      92.7\n",
       "9                          Yue Chinese                      85.2"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "table = soup.find_all('table',{'class':'wikitable sortable'})[0]\n",
    "rows = table.find_all('tr')\n",
    "rows = [row.text.strip().split(\"\\n\") for row in rows]\n",
    "colnames = rows[0]\n",
    "data = rows[1:]\n",
    "\n",
    "df = pd.DataFrame(data, columns=colnames)\n",
    "df[['Language', 'Native Speakers(millions)']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
